# Speech LLM ASR

## Удиртгал

Унших дараалал:

1. Data-Preprocessing
2. Whisper-tiny Mozilla Common Voice Fine-Tuning

> Энэ notebook-г google-colab дээр ажиллуулсан болно. [Colab](
https://colab.research.google.com/drive/11wUF8JnJGhIW2e7NI77uPmr5lK0jLFmd?usp=sharing
)

Надад эхлээд Speech LLM-ын ерөнхий ойлголт авах хэрэг байсан тул. [Jing Peng 2024](https://arxiv.org/abs/2410.18908v3) судалгааны өгүүллийг уншиж таскаа эхэлсэн. Speech LLM бол Large-Language-Model-уудын Multi-Modality урсгал trend-ын нэг юм. Өөрөөр хэлбэл Speech LLM-ын гол асуулт бол "хэрхэн дууны дата-г хэлний модел-д ойлгуулах вэ?" юм.

Анхны оролдлгуудын нэг нь **Cascading Model** буюу Acoustic-Model-оос гарсан текст дээр LLM нэмэлт байдлаар ажиллах. Acoustic-Model-ын (high-level) текст дээр ажиллахаас илүүтэйгээр embedding (tensor) эсвэл token түвшинд LLM-тай харьцвал тухайн уншигчийн сэтгэл хөдлөл, дуу чимээ, хөгжим, дуудлага, өнгө аяс зэргийг алдахгүйгээр LLM-д хүргэх боломжтой байдаг байна. Энэ төрлийн моделуудыг нийтэд нь **SpeechLLM** гэдэг. SpeechLLM-г ерөнхийдөө дараах байдлаар сургадаг.


![](attachment:image.png)


- Эх сурвалж : [Jing Peng 2024](https://arxiv.org/abs/2410.18908v3)

Миний таскийн хүрээнд, мөн *Mozilla Common Voice* датасетийн хүрээнд дээрх диаграммаас харвал текст промпт тогтмол буюу машин сургалтанд нөлөөлөхгүй гэсэн үг. Түүнчлэн LLM-ээс гарах гаралт зөвхөн уншигчийн юу хэлснийг таамаглах зорилготой байна. Гэхдээ энэ таскийн дашрамд ерөнхий тохиолдолд SpeechLLM-г хэрхэн сургах талаар мэдмээр байгаа тул текст промпт-г юу ч гэсэн орхилгүй, тогтмолоор тодорхойлов.

```python
prompt_tokens = llm_tokenizer("Юу гэж хэлсэн бэ?", return_tensors="pt").input_ids.to(device)
prompt_embeds = embed_layer(prompt_tokens) # "Юу гэж хэлсэн бэ?" embedding
```

SpeechLLM-н гол асуулт болох "хэрхэн аудио датаг хэлний моделд ойлгуулах вэ?" асуултын хариуд судлаачид проекц (шугаман функц) ашигладаг тул Whisper моделын хэмжээсээс Qwen2.5 моделын хэмжээс рүү буулгах матриц болох проекцийг энэхүү таскын хүрээнд сургана.

- Яагаад whisper-г сонгосон бэ? 
    - Учир нь надад зөвхөн 3000-аад мөр дата, ойролцоогоор 10 цагийн бичлэг байгаагийн хүрээнд анхнаасаа Монгол, Уйгар хэлэнд сайн (сургагдсан) суурь модел сонгосон нь whisper байв.
- Яагаад Qwen2.5?
    - SpeechLLM-г сургахдаа ихэнхдээ Acoustic model, Language model-оо хөлдөөдөг (сургадаггүй) тиймээс Монгол хэлэндээ сайн байх, жижиг LLM хэрэгтэй болсон ба Qwen-Audio SpeechLLM-ын судалгааны өгүүлэллээс сурвалжлахад амар байлгах зорилгоор  Qwen2.5-г сонгов.

- Яагаад зүгээр шууд Qwen-Audio-г fine tune хийгээгүй вэ?
    - Энэхүү таск аль эсвэл ажлын зорилго магадгүй зөвхөн ажилладаг л модел deploy хийх байсан байж магадгүй ээ. Гэхдээ acoustic model, language model-тойгоо tensor түвшинд ажиллаж, сургалт хийж, хөгжүүлэлт хийх юм бол, дараа дараагийн шинэ SpeechLLM paradigm аль эсвэл өгүүлэллийг implement хийхэд хялбаршуулж өгнө гэж үзэж байна.
    
## Төлөвлөгөө

0. Mozilla Common Voice Mongolia dataset-г preprocess хийж Whisper-тай ажиллахад тохиромжтой болгох
1. Whisper tiny-гын Монгол хэл (Mozilla dataset) дээрх performance-г сайжруулах зорилгоор 10-epoch fine-tuning хийх
2. Qwen tokenizer-аар тесктийг, fine-tuned whisper-tiny-гаар аудиог диск-д хадгалах
    - DataLoader-аас дата татах тутамд encoding process болох тул сургах цагийг багасгах зорилгоор (аудио, текст) датаг машин сургалтаас өмнө preprocess хийв.
3. SpeechLLM  сургалтанд зөвхөн проекц буюу шугаман  neural-network-г сургах ба дуу, хэлний моделуудыг хөлдөөнө.

    - Input: Embedding = [ projector_embedding | prompt_embedding | masked_target_embedding_zeroes]
    - Output: Token = [ -100, -100, ..., -100 (гарах токен харгалзахгүй) | target_tokens]
    
    - Loss: $\mathcal{L} = -\sum\limits_{i} \text{logits}( t_i \text{ токен (уншигчийн юу хэлсэн)} | \text{аудио, промпт } t_{<i})$
    
> Миний арга [Qwen-Audio](https://github.com/QwenLM/Qwen-Audio)-гийн кодыг харахад өөр байгааг ажиглассан ч яг юу өөр байгааг мөн яг адилхан зорилго сургаж байгаа эсэхийг мэдэж чадаагүй.

## Үр дүн

Харамсалтай нь миний сургасан SpeechLLM, Whisper-tiny-аас ч доогуур үзүүлэлтэй сургагдлаа. Гэхдээ энэ таскийн үр дүнд, SpeechLLM-г tensor-ын түвшинд хэрхэн сургах, ерөнхий ажлын шугамыг тодорхойлж гаргаж чадсан гэж үзэж байна.

Хэрэв надад өшөө цаг хугацаа байсан бол, миний арга яг Qwen-Audio-тай юугаараа зөрж байгааг тодорхойлоод, тодорхой бусад моделуудтай ASR таскийн хүрээнд benchmarking хийх гэсэн бодолтой байна.

Түүнчлэн, LLM-тай ярилцахад LLM-ын гаралтыг embedding/token-ын түвшинд Vocoder-оор дуу оруулснаар зөвхөн текст уншаад зогсохгүй тухайн уншлагаа өнгө аяс, аялага, сэтгэл хөдлөлтэйгээр илэрхийлэх боломжтой. Twitch-д [Neurosama](https://www.twitch.tv/neurosama) гэдэг сэтгэл хөдлөлөө, SpeechLLM, Vocoder stack-аар илэрхийлдэг AI хүмүүсийн анхаарлыг татаж сая [2026 оны 1-сард бүх цагийн хамгийн их subscriber-тай 3-дугаар channel болсон байсан](https://twitchtracker.com/subscribers/all-time).
